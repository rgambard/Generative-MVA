{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fb87f02-c727-4b64-8e16-118800d61da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.9374e-01, 9.9322e-01, 9.9269e-01, 9.9213e-01, 9.9156e-01, 9.9096e-01,\n",
      "        9.9035e-01, 9.8972e-01, 9.8907e-01, 9.8840e-01, 9.8771e-01, 9.8700e-01,\n",
      "        9.8627e-01, 9.8553e-01, 9.8476e-01, 9.8398e-01, 9.8317e-01, 9.8235e-01,\n",
      "        9.8151e-01, 9.8065e-01, 9.7977e-01, 9.7887e-01, 9.7795e-01, 9.7702e-01,\n",
      "        9.7606e-01, 9.7509e-01, 9.7410e-01, 9.7309e-01, 9.7206e-01, 9.7102e-01,\n",
      "        9.6995e-01, 9.6887e-01, 9.6777e-01, 9.6665e-01, 9.6551e-01, 9.6436e-01,\n",
      "        9.6319e-01, 9.6200e-01, 9.6079e-01, 9.5956e-01, 9.5832e-01, 9.5706e-01,\n",
      "        9.5578e-01, 9.5449e-01, 9.5318e-01, 9.5185e-01, 9.5050e-01, 9.4914e-01,\n",
      "        9.4776e-01, 9.4636e-01, 9.4494e-01, 9.4351e-01, 9.4207e-01, 9.4060e-01,\n",
      "        9.3912e-01, 9.3762e-01, 9.3611e-01, 9.3458e-01, 9.3304e-01, 9.3147e-01,\n",
      "        9.2990e-01, 9.2830e-01, 9.2669e-01, 9.2507e-01, 9.2343e-01, 9.2177e-01,\n",
      "        9.2010e-01, 9.1841e-01, 9.1671e-01, 9.1500e-01, 9.1326e-01, 9.1152e-01,\n",
      "        9.0976e-01, 9.0798e-01, 9.0619e-01, 9.0438e-01, 9.0256e-01, 9.0073e-01,\n",
      "        8.9888e-01, 8.9702e-01, 8.9514e-01, 8.9325e-01, 8.9135e-01, 8.8943e-01,\n",
      "        8.8750e-01, 8.8555e-01, 8.8359e-01, 8.8162e-01, 8.7964e-01, 8.7764e-01,\n",
      "        8.7563e-01, 8.7360e-01, 8.7157e-01, 8.6952e-01, 8.6746e-01, 8.6538e-01,\n",
      "        8.6330e-01, 8.6120e-01, 8.5909e-01, 8.5697e-01, 8.5483e-01, 8.5269e-01,\n",
      "        8.5053e-01, 8.4836e-01, 8.4618e-01, 8.4399e-01, 8.4179e-01, 8.3957e-01,\n",
      "        8.3735e-01, 8.3511e-01, 8.3287e-01, 8.3061e-01, 8.2834e-01, 8.2606e-01,\n",
      "        8.2378e-01, 8.2148e-01, 8.1917e-01, 8.1685e-01, 8.1453e-01, 8.1219e-01,\n",
      "        8.0984e-01, 8.0749e-01, 8.0512e-01, 8.0275e-01, 8.0037e-01, 7.9797e-01,\n",
      "        7.9557e-01, 7.9316e-01, 7.9075e-01, 7.8832e-01, 7.8589e-01, 7.8344e-01,\n",
      "        7.8099e-01, 7.7854e-01, 7.7607e-01, 7.7360e-01, 7.7111e-01, 7.6863e-01,\n",
      "        7.6613e-01, 7.6363e-01, 7.6112e-01, 7.5860e-01, 7.5608e-01, 7.5354e-01,\n",
      "        7.5101e-01, 7.4846e-01, 7.4591e-01, 7.4336e-01, 7.4080e-01, 7.3823e-01,\n",
      "        7.3565e-01, 7.3308e-01, 7.3049e-01, 7.2790e-01, 7.2530e-01, 7.2270e-01,\n",
      "        7.2010e-01, 7.1749e-01, 7.1487e-01, 7.1225e-01, 7.0963e-01, 7.0700e-01,\n",
      "        7.0436e-01, 7.0172e-01, 6.9908e-01, 6.9644e-01, 6.9379e-01, 6.9113e-01,\n",
      "        6.8847e-01, 6.8581e-01, 6.8315e-01, 6.8048e-01, 6.7781e-01, 6.7514e-01,\n",
      "        6.7246e-01, 6.6978e-01, 6.6710e-01, 6.6441e-01, 6.6173e-01, 6.5904e-01,\n",
      "        6.5635e-01, 6.5365e-01, 6.5096e-01, 6.4826e-01, 6.4556e-01, 6.4286e-01,\n",
      "        6.4016e-01, 6.3745e-01, 6.3475e-01, 6.3204e-01, 6.2934e-01, 6.2663e-01,\n",
      "        6.2392e-01, 6.2121e-01, 6.1850e-01, 6.1579e-01, 6.1308e-01, 6.1037e-01,\n",
      "        6.0765e-01, 6.0494e-01, 6.0223e-01, 5.9952e-01, 5.9681e-01, 5.9410e-01,\n",
      "        5.9139e-01, 5.8868e-01, 5.8597e-01, 5.8326e-01, 5.8055e-01, 5.7785e-01,\n",
      "        5.7514e-01, 5.7244e-01, 5.6973e-01, 5.6703e-01, 5.6433e-01, 5.6164e-01,\n",
      "        5.5894e-01, 5.5624e-01, 5.5355e-01, 5.5086e-01, 5.4817e-01, 5.4549e-01,\n",
      "        5.4280e-01, 5.4012e-01, 5.3744e-01, 5.3476e-01, 5.3209e-01, 5.2942e-01,\n",
      "        5.2675e-01, 5.2409e-01, 5.2142e-01, 5.1876e-01, 5.1611e-01, 5.1346e-01,\n",
      "        5.1081e-01, 5.0816e-01, 5.0552e-01, 5.0288e-01, 5.0024e-01, 4.9761e-01,\n",
      "        4.9499e-01, 4.9236e-01, 4.8974e-01, 4.8713e-01, 4.8452e-01, 4.8191e-01,\n",
      "        4.7931e-01, 4.7671e-01, 4.7412e-01, 4.7153e-01, 4.6895e-01, 4.6637e-01,\n",
      "        4.6380e-01, 4.6123e-01, 4.5867e-01, 4.5611e-01, 4.5355e-01, 4.5101e-01,\n",
      "        4.4846e-01, 4.4593e-01, 4.4340e-01, 4.4087e-01, 4.3835e-01, 4.3583e-01,\n",
      "        4.3332e-01, 4.3082e-01, 4.2832e-01, 4.2583e-01, 4.2335e-01, 4.2087e-01,\n",
      "        4.1839e-01, 4.1593e-01, 4.1347e-01, 4.1101e-01, 4.0856e-01, 4.0612e-01,\n",
      "        4.0369e-01, 4.0126e-01, 3.9883e-01, 3.9642e-01, 3.9401e-01, 3.9161e-01,\n",
      "        3.8921e-01, 3.8683e-01, 3.8444e-01, 3.8207e-01, 3.7970e-01, 3.7734e-01,\n",
      "        3.7499e-01, 3.7264e-01, 3.7031e-01, 3.6798e-01, 3.6565e-01, 3.6334e-01,\n",
      "        3.6103e-01, 3.5872e-01, 3.5643e-01, 3.5414e-01, 3.5187e-01, 3.4959e-01,\n",
      "        3.4733e-01, 3.4508e-01, 3.4283e-01, 3.4059e-01, 3.3836e-01, 3.3613e-01,\n",
      "        3.3391e-01, 3.3171e-01, 3.2951e-01, 3.2731e-01, 3.2513e-01, 3.2295e-01,\n",
      "        3.2078e-01, 3.1862e-01, 3.1647e-01, 3.1433e-01, 3.1219e-01, 3.1007e-01,\n",
      "        3.0795e-01, 3.0584e-01, 3.0374e-01, 3.0164e-01, 2.9956e-01, 2.9748e-01,\n",
      "        2.9541e-01, 2.9335e-01, 2.9130e-01, 2.8926e-01, 2.8722e-01, 2.8520e-01,\n",
      "        2.8318e-01, 2.8117e-01, 2.7917e-01, 2.7718e-01, 2.7520e-01, 2.7323e-01,\n",
      "        2.7126e-01, 2.6931e-01, 2.6736e-01, 2.6542e-01, 2.6349e-01, 2.6157e-01,\n",
      "        2.5966e-01, 2.5775e-01, 2.5586e-01, 2.5397e-01, 2.5210e-01, 2.5023e-01,\n",
      "        2.4837e-01, 2.4652e-01, 2.4468e-01, 2.4284e-01, 2.4102e-01, 2.3920e-01,\n",
      "        2.3740e-01, 2.3560e-01, 2.3381e-01, 2.3203e-01, 2.3026e-01, 2.2850e-01,\n",
      "        2.2675e-01, 2.2501e-01, 2.2327e-01, 2.2155e-01, 2.1983e-01, 2.1812e-01,\n",
      "        2.1642e-01, 2.1473e-01, 2.1305e-01, 2.1138e-01, 2.0972e-01, 2.0806e-01,\n",
      "        2.0642e-01, 2.0478e-01, 2.0315e-01, 2.0153e-01, 1.9992e-01, 1.9832e-01,\n",
      "        1.9673e-01, 1.9515e-01, 1.9357e-01, 1.9201e-01, 1.9045e-01, 1.8890e-01,\n",
      "        1.8736e-01, 1.8583e-01, 1.8431e-01, 1.8280e-01, 1.8129e-01, 1.7980e-01,\n",
      "        1.7831e-01, 1.7683e-01, 1.7537e-01, 1.7391e-01, 1.7245e-01, 1.7101e-01,\n",
      "        1.6958e-01, 1.6815e-01, 1.6673e-01, 1.6533e-01, 1.6393e-01, 1.6254e-01,\n",
      "        1.6115e-01, 1.5978e-01, 1.5841e-01, 1.5706e-01, 1.5571e-01, 1.5437e-01,\n",
      "        1.5304e-01, 1.5171e-01, 1.5040e-01, 1.4909e-01, 1.4779e-01, 1.4650e-01,\n",
      "        1.4522e-01, 1.4395e-01, 1.4269e-01, 1.4143e-01, 1.4018e-01, 1.3894e-01,\n",
      "        1.3771e-01, 1.3649e-01, 1.3527e-01, 1.3406e-01, 1.3286e-01, 1.3167e-01,\n",
      "        1.3049e-01, 1.2932e-01, 1.2815e-01, 1.2699e-01, 1.2584e-01, 1.2470e-01,\n",
      "        1.2356e-01, 1.2243e-01, 1.2131e-01, 1.2020e-01, 1.1910e-01, 1.1800e-01,\n",
      "        1.1691e-01, 1.1583e-01, 1.1476e-01, 1.1369e-01, 1.1264e-01, 1.1159e-01,\n",
      "        1.1054e-01, 1.0951e-01, 1.0848e-01, 1.0746e-01, 1.0645e-01, 1.0544e-01,\n",
      "        1.0445e-01, 1.0346e-01, 1.0247e-01, 1.0150e-01, 1.0053e-01, 9.9567e-02,\n",
      "        9.8613e-02, 9.7667e-02, 9.6727e-02, 9.5794e-02, 9.4869e-02, 9.3950e-02,\n",
      "        9.3039e-02, 9.2134e-02, 9.1237e-02, 9.0346e-02, 8.9463e-02, 8.8586e-02,\n",
      "        8.7716e-02, 8.6853e-02, 8.5996e-02, 8.5146e-02, 8.4303e-02, 8.3467e-02,\n",
      "        8.2637e-02, 8.1814e-02, 8.0998e-02, 8.0188e-02, 7.9384e-02, 7.8587e-02,\n",
      "        7.7797e-02, 7.7012e-02, 7.6235e-02, 7.5463e-02, 7.4698e-02, 7.3939e-02,\n",
      "        7.3186e-02, 7.2440e-02, 7.1700e-02, 7.0966e-02, 7.0238e-02, 6.9516e-02,\n",
      "        6.8800e-02, 6.8090e-02, 6.7386e-02, 6.6688e-02, 6.5996e-02, 6.5309e-02,\n",
      "        6.4629e-02, 6.3954e-02, 6.3285e-02, 6.2622e-02, 6.1965e-02, 6.1313e-02,\n",
      "        6.0667e-02, 6.0026e-02, 5.9391e-02, 5.8762e-02, 5.8138e-02, 5.7520e-02,\n",
      "        5.6907e-02, 5.6299e-02, 5.5697e-02, 5.5100e-02, 5.4508e-02, 5.3922e-02,\n",
      "        5.3341e-02, 5.2765e-02, 5.2194e-02, 5.1628e-02, 5.1068e-02, 5.0513e-02,\n",
      "        4.9962e-02, 4.9417e-02, 4.8876e-02, 4.8341e-02, 4.7810e-02, 4.7284e-02,\n",
      "        4.6764e-02, 4.6247e-02, 4.5736e-02, 4.5230e-02, 4.4728e-02, 4.4231e-02,\n",
      "        4.3738e-02, 4.3250e-02, 4.2767e-02, 4.2288e-02, 4.1814e-02, 4.1344e-02,\n",
      "        4.0879e-02, 4.0418e-02, 3.9961e-02, 3.9509e-02, 3.9061e-02, 3.8618e-02,\n",
      "        3.8178e-02, 3.7743e-02, 3.7312e-02, 3.6886e-02, 3.6463e-02, 3.6045e-02,\n",
      "        3.5631e-02, 3.5220e-02, 3.4814e-02, 3.4412e-02, 3.4014e-02, 3.3619e-02,\n",
      "        3.3229e-02, 3.2842e-02, 3.2460e-02, 3.2081e-02, 3.1705e-02, 3.1334e-02,\n",
      "        3.0966e-02, 3.0603e-02, 3.0242e-02, 2.9886e-02, 2.9533e-02, 2.9183e-02,\n",
      "        2.8837e-02, 2.8495e-02, 2.8156e-02, 2.7821e-02, 2.7489e-02, 2.7160e-02,\n",
      "        2.6835e-02, 2.6513e-02, 2.6195e-02, 2.5879e-02, 2.5567e-02, 2.5259e-02,\n",
      "        2.4953e-02, 2.4651e-02, 2.4352e-02, 2.4056e-02, 2.3763e-02, 2.3474e-02,\n",
      "        2.3187e-02, 2.2903e-02, 2.2623e-02, 2.2345e-02, 2.2071e-02, 2.1799e-02,\n",
      "        2.1530e-02, 2.1264e-02, 2.1001e-02, 2.0741e-02, 2.0484e-02, 2.0229e-02,\n",
      "        1.9977e-02, 1.9728e-02, 1.9482e-02, 1.9238e-02, 1.8997e-02, 1.8758e-02,\n",
      "        1.8523e-02, 1.8289e-02, 1.8059e-02, 1.7831e-02, 1.7605e-02, 1.7382e-02,\n",
      "        1.7161e-02, 1.6943e-02, 1.6728e-02, 1.6514e-02, 1.6304e-02, 1.6095e-02,\n",
      "        1.5889e-02, 1.5685e-02, 1.5484e-02, 1.5284e-02, 1.5087e-02, 1.4893e-02,\n",
      "        1.4700e-02, 1.4510e-02, 1.4321e-02, 1.4135e-02, 1.3952e-02, 1.3770e-02,\n",
      "        1.3590e-02, 1.3413e-02, 1.3237e-02, 1.3064e-02, 1.2892e-02, 1.2723e-02,\n",
      "        1.2555e-02, 1.2389e-02, 1.2226e-02, 1.2064e-02, 1.1904e-02, 1.1746e-02,\n",
      "        1.1590e-02, 1.1436e-02, 1.1284e-02, 1.1133e-02, 1.0984e-02, 1.0837e-02,\n",
      "        1.0692e-02, 1.0548e-02, 1.0407e-02, 1.0266e-02, 1.0128e-02, 9.9911e-03,\n",
      "        9.8560e-03, 9.7225e-03, 9.5906e-03, 9.4603e-03, 9.3316e-03, 9.2044e-03,\n",
      "        9.0788e-03, 8.9548e-03, 8.8322e-03, 8.7112e-03, 8.5916e-03, 8.4735e-03,\n",
      "        8.3569e-03, 8.2417e-03, 8.1279e-03, 8.0155e-03, 7.9046e-03, 7.7950e-03,\n",
      "        7.6867e-03, 7.5798e-03, 7.4743e-03, 7.3701e-03, 7.2672e-03, 7.1655e-03,\n",
      "        7.0652e-03, 6.9661e-03, 6.8683e-03, 6.7717e-03, 6.6763e-03, 6.5821e-03,\n",
      "        6.4892e-03, 6.3974e-03, 6.3068e-03, 6.2173e-03, 6.1290e-03, 6.0419e-03,\n",
      "        5.9558e-03, 5.8709e-03, 5.7870e-03, 5.7042e-03, 5.6225e-03, 5.5419e-03,\n",
      "        5.4623e-03, 5.3837e-03, 5.3062e-03, 5.2297e-03, 5.1541e-03, 5.0796e-03,\n",
      "        5.0060e-03, 4.9334e-03, 4.8618e-03, 4.7911e-03, 4.7213e-03, 4.6525e-03,\n",
      "        4.5845e-03, 4.5175e-03, 4.4514e-03, 4.3861e-03, 4.3217e-03, 4.2582e-03,\n",
      "        4.1955e-03, 4.1336e-03, 4.0726e-03, 4.0124e-03, 3.9530e-03, 3.8945e-03,\n",
      "        3.8367e-03, 3.7796e-03, 3.7234e-03, 3.6679e-03, 3.6132e-03, 3.5592e-03,\n",
      "        3.5060e-03, 3.4534e-03, 3.4016e-03, 3.3506e-03, 3.3002e-03, 3.2505e-03,\n",
      "        3.2014e-03, 3.1531e-03, 3.1054e-03, 3.0584e-03, 3.0120e-03, 2.9663e-03,\n",
      "        2.9212e-03, 2.8768e-03, 2.8329e-03, 2.7897e-03, 2.7471e-03, 2.7051e-03,\n",
      "        2.6636e-03, 2.6228e-03, 2.5825e-03, 2.5428e-03, 2.5036e-03, 2.4650e-03,\n",
      "        2.4270e-03, 2.3894e-03, 2.3525e-03, 2.3160e-03, 2.2801e-03, 2.2446e-03,\n",
      "        2.2097e-03, 2.1753e-03, 2.1414e-03, 2.1079e-03, 2.0750e-03, 2.0425e-03,\n",
      "        2.0104e-03, 1.9789e-03, 1.9478e-03, 1.9171e-03, 1.8869e-03, 1.8572e-03,\n",
      "        1.8278e-03, 1.7989e-03, 1.7704e-03, 1.7423e-03, 1.7147e-03, 1.6874e-03,\n",
      "        1.6606e-03, 1.6341e-03, 1.6080e-03, 1.5823e-03, 1.5570e-03, 1.5321e-03,\n",
      "        1.5075e-03, 1.4833e-03, 1.4595e-03, 1.4360e-03, 1.4128e-03, 1.3900e-03,\n",
      "        1.3676e-03, 1.3455e-03, 1.3237e-03, 1.3022e-03, 1.2811e-03, 1.2602e-03,\n",
      "        1.2397e-03, 1.2195e-03, 1.1996e-03, 1.1800e-03, 1.1607e-03, 1.1417e-03,\n",
      "        1.1230e-03, 1.1046e-03, 1.0864e-03, 1.0686e-03, 1.0509e-03, 1.0336e-03,\n",
      "        1.0165e-03, 9.9974e-04, 9.8319e-04, 9.6689e-04, 9.5085e-04, 9.3505e-04,\n",
      "        9.1950e-04, 9.0419e-04, 8.8911e-04, 8.7427e-04, 8.5966e-04, 8.4527e-04,\n",
      "        8.3111e-04, 8.1717e-04, 8.0345e-04, 7.8994e-04, 7.7664e-04, 7.6355e-04,\n",
      "        7.5067e-04, 7.3799e-04, 7.2551e-04, 7.1322e-04, 7.0113e-04, 6.8923e-04,\n",
      "        6.7752e-04, 6.6600e-04, 6.5465e-04, 6.4349e-04, 6.3250e-04, 6.2169e-04,\n",
      "        6.1106e-04, 6.0059e-04, 5.9029e-04, 5.8015e-04, 5.7018e-04, 5.6036e-04,\n",
      "        5.5071e-04, 5.4121e-04, 5.3186e-04, 5.2266e-04, 5.1362e-04, 5.0472e-04,\n",
      "        4.9596e-04, 4.8734e-04, 4.7887e-04, 4.7053e-04, 4.6233e-04, 4.5426e-04,\n",
      "        4.4633e-04, 4.3852e-04, 4.3084e-04, 4.2329e-04, 4.1586e-04, 4.0855e-04,\n",
      "        4.0137e-04, 3.9430e-04, 3.8735e-04, 3.8051e-04, 3.7379e-04, 3.6718e-04,\n",
      "        3.6067e-04, 3.5428e-04, 3.4799e-04, 3.4181e-04, 3.3573e-04, 3.2975e-04,\n",
      "        3.2387e-04, 3.1809e-04, 3.1240e-04, 3.0682e-04, 3.0132e-04, 2.9592e-04,\n",
      "        2.9061e-04, 2.8539e-04, 2.8025e-04, 2.7521e-04, 2.7024e-04, 2.6537e-04,\n",
      "        2.6057e-04, 2.5586e-04, 2.5123e-04, 2.4667e-04, 2.4220e-04, 2.3780e-04,\n",
      "        2.3347e-04, 2.2922e-04, 2.2504e-04, 2.2094e-04, 2.1690e-04, 2.1293e-04,\n",
      "        2.0904e-04, 2.0520e-04, 2.0144e-04, 1.9774e-04, 1.9410e-04, 1.9053e-04,\n",
      "        1.8702e-04, 1.8357e-04, 1.8018e-04, 1.7685e-04, 1.7358e-04, 1.7036e-04,\n",
      "        1.6720e-04, 1.6410e-04, 1.6105e-04, 1.5805e-04, 1.5511e-04, 1.5222e-04,\n",
      "        1.4937e-04, 1.4658e-04, 1.4384e-04, 1.4115e-04, 1.3850e-04, 1.3590e-04,\n",
      "        1.3335e-04, 1.3084e-04, 1.2838e-04, 1.2596e-04, 1.2358e-04, 1.2125e-04,\n",
      "        1.1896e-04, 1.1671e-04, 1.1450e-04, 1.1232e-04, 1.1019e-04, 1.0810e-04,\n",
      "        1.0604e-04, 1.0402e-04, 1.0204e-04, 1.0009e-04, 9.8180e-05, 9.6302e-05,\n",
      "        9.4459e-05, 9.2649e-05, 9.0871e-05, 8.9126e-05, 8.7413e-05, 8.5731e-05,\n",
      "        8.4080e-05, 8.2458e-05, 8.0867e-05, 7.9304e-05, 7.7770e-05, 7.6264e-05,\n",
      "        7.4786e-05, 7.3335e-05, 7.1911e-05, 7.0513e-05, 6.9140e-05, 6.7793e-05,\n",
      "        6.6471e-05, 6.5174e-05, 6.3900e-05, 6.2650e-05, 6.1423e-05, 6.0219e-05,\n",
      "        5.9038e-05, 5.7878e-05, 5.6740e-05, 5.5623e-05, 5.4527e-05, 5.3452e-05,\n",
      "        5.2397e-05, 5.1361e-05, 5.0346e-05, 4.9349e-05, 4.8371e-05, 4.7411e-05,\n",
      "        4.6469e-05, 4.5545e-05, 4.4639e-05, 4.3750e-05, 4.2877e-05, 4.2022e-05,\n",
      "        4.1182e-05, 4.0358e-05])\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from unet import ResidualUNet, Denoiser\n",
    "from utils import save_image, dotdict\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "#sigmas = 3*torch.pow(torch.ones(50)*0.9,torch.arange(50))\n",
    "betat = torch.linspace(1e-4,0.02,1000)\n",
    "alphat = 1-betat\n",
    "alphabart = [1]\n",
    "for alpha in alphat:\n",
    "    alphabart.append(alphabart[-1]*alpha)\n",
    "\n",
    "alphabart = torch.Tensor(alphabart[1:])\n",
    "\n",
    "\n",
    "betat = betat[20:]\n",
    "alphat = alphat[20:]\n",
    "alphabart = alphabart[20:]\n",
    "print(alphabart)\n",
    "\n",
    "\n",
    "def forward(data, model, alphasb_batch = None):\n",
    "    im = data\n",
    "\n",
    "    # parameters of the langevin dynamics steps\n",
    "    ind_randoms= torch.randint(0, alphat.shape[0], (data.shape[0],), device = data.device)\n",
    "    \n",
    "    noise_in = torch.randn_like(im)\n",
    "    if alphasb_batch is None:\n",
    "        alphasb_batch = alphabart[ind_randoms]\n",
    "\n",
    "        im_input = (torch.sqrt(1-alphasb_batch)[:,None,None,None]*noise_in+torch.sqrt(alphasb_batch)[:,None,None,None]*im)\n",
    "    else :\n",
    "        im_input = im\n",
    "    # we append the sigmas to the model input as a new dimension of the image\n",
    "    features_sigma = torch.sin(ind_randoms[:,None,None,None]/1000*(torch.arange(im.shape[2], device = im.device)[None,None,:,None]*20+200*torch.arange(im.shape[3], device = im.device)[None,None,None,:]))/0.70 # we encode the sigmas into sinosiudals encodings \n",
    "    #mod_input = torch.cat((im_input_norm, features_sigma, features_mean, features_std), dim=1)\n",
    "    mod_input = torch.cat((im_input, features_sigma), dim=1)\n",
    "    #(noise_in+im)/sqrt(2) = pred\n",
    "    #pred-torch.sqrt(1-alphasb_batch)*im_input=\n",
    "    #noise+  \n",
    "    pred_eps = model(mod_input)\n",
    "    score = -pred_eps/torch.sqrt(1-alphasb_batch)[:,None,None,None]\n",
    "    # corrected image \n",
    "    im_corrected = (im_input-torch.sqrt(1-alphasb_batch)[:,None,None,None]*pred_eps)/torch.sqrt(alphasb_batch)[:,None,None,None]\n",
    "\n",
    "    square_norm = torch.sum((pred_eps-noise_in)**2,(1,2,3)) # square norm of loss per image\n",
    "    loss = square_norm.sum()\n",
    "    return loss, im_input, im_corrected, pred_eps, score\n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "     \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss, im_input, im_corrected, pred_eps, score= forward(data, model)\n",
    "\n",
    "        loss.backward()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_idx+1) % args.log_interval  == 0:\n",
    "            running_loss = running_loss/(args.log_interval*args.batch_size)\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), running_loss))\n",
    "            running_loss = 0\n",
    "            if args.dry_run:\n",
    "                break\n",
    "\n",
    "nid= \"vnoise\"\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    ctime = time.time()\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            im = data\n",
    "            loss, im_input, corr, pred_eps, score= forward(data, model)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set {:.4f} Average loss: {:.4f} \\n'.format(time.time()-ctime,test_loss))\n",
    "    orig = im\n",
    "    noisy = im_input\n",
    "    save_image(noisy[:10],\"im/noisy.jpg\")\n",
    "    save_image(corr[:10],\"im/corrected.jpg\")\n",
    "    save_image(orig[:10],\"im/originals.jpg\")\n",
    "    gen_shape = list(im.shape)\n",
    "    gen_shape[0] = 32\n",
    "    gen_im_sde = sample_sde(model, device, gen_shape)\n",
    "    save_image(gen_im_sde, \"im/generatedsde.jpg\")\n",
    "    gen_im_ddpm = sampleDDPM(model, device, gen_shape)\n",
    "    save_image(gen_im_ddpm, \"im/generatedddpm.jpg\")\n",
    "    gen_im_ode = sample_ode(model, device, gen_shape)\n",
    "    save_image(gen_im_ode, \"im/generatedode.jpg\")\n",
    "\n",
    "\n",
    "def sampleDDPM(model,device, im_shape):\n",
    "    print(\"generating images...\")\n",
    "    with torch.no_grad():\n",
    "        xt = torch.randn(im_shape, device = device)\n",
    "        for t in range(alphabart.shape[0]-1,1,-1):\n",
    "            zt = torch.randn_like(xt)\n",
    "            alpha = alphat[t]\n",
    "            beta = 1-alpha\n",
    "            alphasb_batch = torch.ones((xt.shape[0],), device=  device)*alphabart[t]\n",
    "            loss, im_input, im_corrected, pred_eps, score= forward(xt, model, alphasb_batch=alphasb_batch)\n",
    "            xt = 1/torch.sqrt(alpha)*(xt - beta/torch.sqrt((1-alphabart[t]))*pred_eps)+torch.sqrt((1-alphabart[t-1])/(1-alphabart[t])*beta)*zt\n",
    "            if t%100==0:\n",
    "                print(t, beta, torch.std(xt), torch.mean(xt))\n",
    "\n",
    "    print(\"images generated ! \")\n",
    "    return xt\n",
    "\n",
    "def sample_ode(self, device, im_shape):\n",
    "    with torch.no_grad():  # avoid backprop wrt model parameters\n",
    "        x = torch.randn(im_shape, device=device)\n",
    "        for t in range(alphabart.shape[0]-1,1,-1):\n",
    "            alpha_t = alphat[t]\n",
    "            beta_t = 1-alpha_t\n",
    "            alphasb_batch = torch.ones((x.shape[0],), device=  device)*alphabart[t]\n",
    "            loss, im_input, im_corrected, pred_eps, score = forward(x, model, alphasb_batch=alphasb_batch)\n",
    "            x = (2-torch.sqrt(1-beta_t))*x+1/2*beta_t*score\n",
    "    return x\n",
    "\n",
    "def sample_sde(self,device, im_shape):\n",
    "    with torch.no_grad():  # avoid backprop wrt model parameters\n",
    "        x = torch.randn(im_shape, device=device)\n",
    "        for t in range(alphabart.shape[0]-1,1,-1):\n",
    "            alpha_t = alphat[t]\n",
    "            beta_t = 1-alpha_t\n",
    "            alphasb_batch = torch.ones((x.shape[0],), device=  device)*alphabart[t]\n",
    "            loss, im_input, im_corrected, pred_eps, score = forward(x, model, alphasb_batch=alphasb_batch)\n",
    "            xp = (2-torch.sqrt(1-beta_t))*x+beta_t*score\n",
    "            noise = torch.randn_like(x)\n",
    "            x = xp + torch.sqrt(beta_t) * noise\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7595ad68-8d9d-460a-bead-c3eded771cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [12736/60000 (21%)]\tLoss: 22.735133\n",
      "Train Epoch: 1 [25536/60000 (43%)]\tLoss: 21.110077\n",
      "Train Epoch: 1 [38336/60000 (64%)]\tLoss: 21.043745\n",
      "Train Epoch: 1 [51136/60000 (85%)]\tLoss: 21.093024\n",
      "Train Epoch: 2 [12736/60000 (21%)]\tLoss: 20.660006\n",
      "Train Epoch: 2 [25536/60000 (43%)]\tLoss: 20.990484\n",
      "Train Epoch: 2 [38336/60000 (64%)]\tLoss: 21.151759\n",
      "Train Epoch: 2 [51136/60000 (85%)]\tLoss: 21.148637\n",
      "Train Epoch: 3 [12736/60000 (21%)]\tLoss: 20.957502\n",
      "Train Epoch: 3 [25536/60000 (43%)]\tLoss: 20.679564\n",
      "Train Epoch: 3 [38336/60000 (64%)]\tLoss: 21.258029\n",
      "Train Epoch: 3 [51136/60000 (85%)]\tLoss: 20.994987\n",
      "\n",
      "Test set 4.2930 Average loss: 21.3489 \n",
      "\n",
      "generating images...\n",
      "900 tensor(0.0184) tensor(0.9998, device='cuda:0') tensor(-0.0105, device='cuda:0')\n",
      "800 tensor(0.0164) tensor(1.0058, device='cuda:0') tensor(-0.0173, device='cuda:0')\n",
      "700 tensor(0.0144) tensor(1.0096, device='cuda:0') tensor(-0.0317, device='cuda:0')\n",
      "600 tensor(0.0125) tensor(1.0141, device='cuda:0') tensor(-0.0369, device='cuda:0')\n",
      "500 tensor(0.0105) tensor(1.0318, device='cuda:0') tensor(-0.0364, device='cuda:0')\n",
      "400 tensor(0.0085) tensor(1.0386, device='cuda:0') tensor(-0.0341, device='cuda:0')\n",
      "300 tensor(0.0065) tensor(1.0174, device='cuda:0') tensor(-0.0261, device='cuda:0')\n",
      "200 tensor(0.0045) tensor(0.9633, device='cuda:0') tensor(-0.0404, device='cuda:0')\n",
      "100 tensor(0.0025) tensor(0.8727, device='cuda:0') tensor(-0.0761, device='cuda:0')\n",
      "images generated ! \n",
      "Train Epoch: 4 [12736/60000 (21%)]\tLoss: 21.293369\n",
      "Train Epoch: 4 [25536/60000 (43%)]\tLoss: 21.178963\n",
      "Train Epoch: 4 [38336/60000 (64%)]\tLoss: 20.884555\n",
      "Train Epoch: 4 [51136/60000 (85%)]\tLoss: 21.033229\n",
      "Train Epoch: 5 [12736/60000 (21%)]\tLoss: 21.168265\n",
      "Train Epoch: 5 [25536/60000 (43%)]\tLoss: 21.291860\n",
      "Train Epoch: 5 [38336/60000 (64%)]\tLoss: 20.408286\n",
      "Train Epoch: 5 [51136/60000 (85%)]\tLoss: 21.195038\n",
      "Train Epoch: 6 [12736/60000 (21%)]\tLoss: 20.902134\n",
      "Train Epoch: 6 [25536/60000 (43%)]\tLoss: 21.219995\n",
      "Train Epoch: 6 [38336/60000 (64%)]\tLoss: 20.985217\n",
      "Train Epoch: 6 [51136/60000 (85%)]\tLoss: 20.954893\n",
      "\n",
      "Test set 4.3032 Average loss: 20.8862 \n",
      "\n",
      "generating images...\n",
      "900 tensor(0.0184) tensor(1.0010, device='cuda:0') tensor(-0.0296, device='cuda:0')\n",
      "800 tensor(0.0164) tensor(0.9937, device='cuda:0') tensor(-0.0439, device='cuda:0')\n",
      "700 tensor(0.0144) tensor(1.0107, device='cuda:0') tensor(-0.0534, device='cuda:0')\n",
      "600 tensor(0.0125) tensor(1.0305, device='cuda:0') tensor(-0.0672, device='cuda:0')\n",
      "500 tensor(0.0105) tensor(1.0537, device='cuda:0') tensor(-0.0711, device='cuda:0')\n",
      "400 tensor(0.0085) tensor(1.0731, device='cuda:0') tensor(-0.0629, device='cuda:0')\n",
      "300 tensor(0.0065) tensor(1.0659, device='cuda:0') tensor(-0.0582, device='cuda:0')\n",
      "200 tensor(0.0045) tensor(1.0176, device='cuda:0') tensor(-0.0704, device='cuda:0')\n",
      "100 tensor(0.0025) tensor(0.9272, device='cuda:0') tensor(-0.0925, device='cuda:0')\n",
      "images generated ! \n",
      "Train Epoch: 7 [12736/60000 (21%)]\tLoss: 21.116642\n",
      "Train Epoch: 7 [25536/60000 (43%)]\tLoss: 20.763197\n",
      "Train Epoch: 7 [38336/60000 (64%)]\tLoss: 21.197052\n",
      "Train Epoch: 7 [51136/60000 (85%)]\tLoss: 20.651709\n",
      "Train Epoch: 8 [12736/60000 (21%)]\tLoss: 20.815260\n",
      "Train Epoch: 8 [25536/60000 (43%)]\tLoss: 20.698400\n",
      "Train Epoch: 8 [38336/60000 (64%)]\tLoss: 20.843611\n",
      "Train Epoch: 8 [51136/60000 (85%)]\tLoss: 21.027504\n",
      "Train Epoch: 9 [12736/60000 (21%)]\tLoss: 20.871839\n",
      "Train Epoch: 9 [25536/60000 (43%)]\tLoss: 20.966622\n",
      "Train Epoch: 9 [38336/60000 (64%)]\tLoss: 21.120405\n",
      "Train Epoch: 9 [51136/60000 (85%)]\tLoss: 20.687720\n",
      "\n",
      "Test set 4.2762 Average loss: 20.7595 \n",
      "\n",
      "generating images...\n",
      "900 tensor(0.0184) tensor(1.0009, device='cuda:0') tensor(-0.0222, device='cuda:0')\n",
      "800 tensor(0.0164) tensor(1.0033, device='cuda:0') tensor(-0.0277, device='cuda:0')\n",
      "700 tensor(0.0144) tensor(0.9997, device='cuda:0') tensor(-0.0272, device='cuda:0')\n",
      "600 tensor(0.0125) tensor(1.0064, device='cuda:0') tensor(-0.0366, device='cuda:0')\n",
      "500 tensor(0.0105) tensor(1.0132, device='cuda:0') tensor(-0.0373, device='cuda:0')\n",
      "400 tensor(0.0085) tensor(1.0131, device='cuda:0') tensor(-0.0480, device='cuda:0')\n",
      "300 tensor(0.0065) tensor(0.9903, device='cuda:0') tensor(-0.0482, device='cuda:0')\n",
      "200 tensor(0.0045) tensor(0.9379, device='cuda:0') tensor(-0.0602, device='cuda:0')\n",
      "100 tensor(0.0025) tensor(0.8464, device='cuda:0') tensor(-0.0950, device='cuda:0')\n",
      "images generated ! \n",
      "Train Epoch: 10 [12736/60000 (21%)]\tLoss: 20.920004\n",
      "Train Epoch: 10 [25536/60000 (43%)]\tLoss: 20.804065\n",
      "Train Epoch: 10 [38336/60000 (64%)]\tLoss: 20.952303\n",
      "Train Epoch: 10 [51136/60000 (85%)]\tLoss: 20.657835\n",
      "Train Epoch: 11 [12736/60000 (21%)]\tLoss: 20.638663\n",
      "Train Epoch: 11 [25536/60000 (43%)]\tLoss: 20.726336\n",
      "Train Epoch: 11 [38336/60000 (64%)]\tLoss: 20.810580\n",
      "Train Epoch: 11 [51136/60000 (85%)]\tLoss: 20.760718\n",
      "Train Epoch: 12 [12736/60000 (21%)]\tLoss: 20.877100\n",
      "Train Epoch: 12 [25536/60000 (43%)]\tLoss: 20.838000\n",
      "Train Epoch: 12 [38336/60000 (64%)]\tLoss: 20.650152\n",
      "Train Epoch: 12 [51136/60000 (85%)]\tLoss: 20.904914\n",
      "\n",
      "Test set 4.2741 Average loss: 20.6169 \n",
      "\n",
      "generating images...\n",
      "900 tensor(0.0184) tensor(1.0005, device='cuda:0') tensor(0.0071, device='cuda:0')\n",
      "800 tensor(0.0164) tensor(1.0017, device='cuda:0') tensor(-0.0143, device='cuda:0')\n",
      "700 tensor(0.0144) tensor(1.0020, device='cuda:0') tensor(-0.0189, device='cuda:0')\n",
      "600 tensor(0.0125) tensor(1.0100, device='cuda:0') tensor(-0.0282, device='cuda:0')\n",
      "500 tensor(0.0105) tensor(1.0210, device='cuda:0') tensor(-0.0246, device='cuda:0')\n",
      "400 tensor(0.0085) tensor(1.0106, device='cuda:0') tensor(-0.0190, device='cuda:0')\n",
      "300 tensor(0.0065) tensor(0.9925, device='cuda:0') tensor(-0.0184, device='cuda:0')\n",
      "200 tensor(0.0045) tensor(0.9409, device='cuda:0') tensor(-0.0227, device='cuda:0')\n",
      "100 tensor(0.0025) tensor(0.8412, device='cuda:0') tensor(-0.0577, device='cuda:0')\n",
      "images generated ! \n",
      "Train Epoch: 13 [12736/60000 (21%)]\tLoss: 20.851016\n",
      "Train Epoch: 13 [25536/60000 (43%)]\tLoss: 20.382561\n",
      "Train Epoch: 13 [38336/60000 (64%)]\tLoss: 21.051489\n",
      "Train Epoch: 13 [51136/60000 (85%)]\tLoss: 20.641837\n",
      "Train Epoch: 14 [12736/60000 (21%)]\tLoss: 20.454134\n",
      "Train Epoch: 14 [25536/60000 (43%)]\tLoss: 20.875384\n",
      "Train Epoch: 14 [38336/60000 (64%)]\tLoss: 20.813772\n",
      "Train Epoch: 14 [51136/60000 (85%)]\tLoss: 20.466915\n",
      "Train Epoch: 15 [12736/60000 (21%)]\tLoss: 20.734200\n",
      "Train Epoch: 15 [25536/60000 (43%)]\tLoss: 20.604733\n",
      "Train Epoch: 15 [38336/60000 (64%)]\tLoss: 20.551387\n",
      "Train Epoch: 15 [51136/60000 (85%)]\tLoss: 20.452426\n",
      "\n",
      "Test set 4.2656 Average loss: 21.2637 \n",
      "\n",
      "generating images...\n",
      "900 tensor(0.0184) tensor(1.0081, device='cuda:0') tensor(-0.0047, device='cuda:0')\n",
      "800 tensor(0.0164) tensor(1.0088, device='cuda:0') tensor(-0.0113, device='cuda:0')\n",
      "700 tensor(0.0144) tensor(1.0052, device='cuda:0') tensor(-0.0124, device='cuda:0')\n",
      "600 tensor(0.0125) tensor(1.0010, device='cuda:0') tensor(-0.0103, device='cuda:0')\n",
      "500 tensor(0.0105) tensor(0.9947, device='cuda:0') tensor(-0.0110, device='cuda:0')\n",
      "400 tensor(0.0085) tensor(0.9832, device='cuda:0') tensor(-0.0104, device='cuda:0')\n",
      "300 tensor(0.0065) tensor(0.9410, device='cuda:0') tensor(-0.0102, device='cuda:0')\n",
      "200 tensor(0.0045) tensor(0.8695, device='cuda:0') tensor(-0.0199, device='cuda:0')\n",
      "100 tensor(0.0025) tensor(0.7482, device='cuda:0') tensor(-0.0573, device='cuda:0')\n",
      "images generated ! \n",
      "Train Epoch: 16 [12736/60000 (21%)]\tLoss: 20.637675\n",
      "Train Epoch: 16 [25536/60000 (43%)]\tLoss: 20.768377\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TEST = False# set to true to load model from disk and only generate to test langevin\n",
    "\n",
    "# Training settings\n",
    "args_dict = {'batch_size' : 64, 'test_batch_size' :64, 'epochs' :1000, 'lr' : 0.0002, 'gamma' : 0.995, 'no_cuda' :False, \n",
    "             'dry_run':False, 'seed': 1, 'log_interval' : 200, 'save_model' :True, 'only_test':False, 'model_path':\"models/denoisermnist.pt\", \n",
    "             'load_model_from_disk':True, 'dataset':\"MNIST\", 'test':False}\n",
    "args = dotdict(args_dict)\n",
    "#parser = argparse.ArgumentParser(description=\"A simple argument parser example.\")\n",
    "\n",
    "# Add arguments\n",
    "#parser.add_argument('--dataset', type=str, required=False, default = 'MNIST', help='Dataset can be one of MNIST, CIFAR, CELEBA')\n",
    "#parser.add_argument('--test', type= str, required = False, help='wether to only test a model, requires path to the testing weights')\n",
    "\n",
    "# Parse the arguments\n",
    "#margs = parser.parse_args()\n",
    "#args.dataset = margs.dataset\n",
    "#if margs.test is not None:\n",
    "#    print(\"TEST\")\n",
    "#    args.test = True\n",
    "#    print(args.test)\n",
    "#    args.model_path = margs.test\n",
    "\n",
    "if args.test:\n",
    "    print(\"TEST\")\n",
    "    args.load_model_from_disk = True\n",
    "    args.only_test = True\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "if use_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "train_kwargs = {'batch_size': args.batch_size}\n",
    "test_kwargs = {'batch_size': args.test_batch_size}\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "                   'pin_memory': True,\n",
    "                   'shuffle': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "dataset = args.dataset\n",
    "if dataset == \"CIFAR\":\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),#mean, std\n",
    "    transforms.RandomHorizontalFlip(p=0.5)\n",
    "    ])\n",
    "\n",
    "    dataset1 = datasets.CIFAR10(root='./data/cifar10', train=True, download=True, transform=transform)\n",
    "    dataset2 = datasets.CIFAR10(root='./data/cifar10', train=False, download=True, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "    model = Denoiser(4,3).to(device)\n",
    "elif dataset == \"CELEBA\":\n",
    "    transform = transforms.Compose([transforms.Resize((64,64)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                    std=[0.5, 0.5, 0.5])])\n",
    "    dataset1 = datasets.CelebA(\"./data/celeba\", split = 'train',download=False, transform=transform)\n",
    "    dataset2 = datasets.CelebA(\"./data/celeba\", split = 'test', download=False, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "    model = Denoiser(3,3).to(device)\n",
    "\n",
    "elif dataset==\"MNIST\":\n",
    "    # loading dataset\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Pad(2),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    dataset1 = datasets.MNIST('./data/mnist', train=True, download=True,\n",
    "    transform=transform)\n",
    "    dataset2 = datasets.MNIST('./data/mnist', train=False,\n",
    "    transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = Denoiser(2,1).to(device)\n",
    "\n",
    "if args.load_model_from_disk:\n",
    "    model.load_state_dict(torch.load(args.model_path, weights_only= True))\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "\n",
    "alphabart= alphabart.to(device)\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    if not args.only_test:\n",
    "        train(args, model , device, train_loader, optimizer, epoch)\n",
    "        scheduler.step()\n",
    "        if args.save_model:\n",
    "            torch.save(model.state_dict(), args.model_path)\n",
    "    if epoch%3 == 0:\n",
    "        test(model,  device, test_loader)\n",
    "\n",
    "if args.save_model:\n",
    "    torch.save(model.state_dict(), args.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed50926-c934-4309-a360-592d2d580487",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
